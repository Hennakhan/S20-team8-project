{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import functools\n",
    "import keras\n",
    "import keras.callbacks\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 5\n",
    "FIGURES_PATH = Path('figures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_acc = functools.partial(keras.metrics.top_k_categorical_accuracy, k=10)\n",
    "top10_acc.__name__ = 'top10_acc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## histories = defaultdict(list)\n",
    "y_tests = defaultdict(list)\n",
    "y_preds = defaultdict(list)\n",
    "histories = defaultdict(list)\n",
    "accuracies = defaultdict(dict)\n",
    "\n",
    "\n",
    "def top_n_accuracy(y_tests, y_preds, n):\n",
    "    num_classes = len(y_preds[0])\n",
    "    top_n_predictions = [y_test in sorted(range(num_classes), key=lambda n: y_pred[n], reverse=True)[:n] for y_pred, y_test in zip(y_preds, y_test)]\n",
    "    return float(sum(top_n_predictions)) / len(y_tests)\n",
    "\n",
    "\n",
    "for num_classes in [25,50,101]:\n",
    "    data_dir = Path(f\"{num_classes}_runs\")\n",
    "    for run in range(1, NUM_FOLDS + 1):\n",
    "        #history = pickle.load((data_dir / f'history_{num_classes}_{run}.pkl').open('rb'))\n",
    "        y_test = pickle.load((data_dir / f'y_test_{num_classes}_{run}.pkl').open('rb'))\n",
    "        y_tests[num_classes].append(y_test)\n",
    "        y_pred = pickle.load((data_dir / f'y_predict_{num_classes}_{run}.pkl').open('rb'))\n",
    "        y_preds[num_classes].append(y_pred)\n",
    "    print(f\"{num_classes} classes:\")\n",
    "    for n in [1,5,10]:\n",
    "        avg_topn_accuracy = sum([top_n_accuracy(y_test, y_pred, n) for y_test, y_pred in zip(y_tests[num_classes], y_preds[num_classes])]) / len(y_tests[num_classes])\n",
    "        print(f\"Top {n} accuracy: {avg_topn_accuracy}\")\n",
    "        accuracies[num_classes][n] = avg_topn_accuracy\n",
    "    f1 = np.mean([f1_score(y_test, np.argmax(y_pred, axis=1), average=\"macro\") for y_test, y_pred in zip(y_tests[num_classes], y_preds[num_classes])])\n",
    "    print(f\"F1 Score: {f1}\")\n",
    "    if num_classes == 101:\n",
    "        print(type(y_preds[num_classes][0][0]))\n",
    "    print(np.array(y_preds[num_classes][0]).shape)\n",
    "    y_pred_top1_all = np.hstack(np.argmax(np.array(y_preds[num_classes]), axis=2))\n",
    "    y_test_all = np.hstack(y_tests[num_classes])\n",
    "    print(y_pred_top1_all.shape)\n",
    "    print(y_test_all.shape)\n",
    "    print(sum(y_pred_top1_all == y_test_all))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame([(num_classes, top_n, accuracy) for num_classes, acc in accuracies.items() for top_n, accuracy in acc.items()], columns=[\"n_classes\", \"Top N\", \"accuracy\"])\n",
    "fig=sns.barplot(x=\"n_classes\", y=\"accuracy\", hue=\"Top N\", data=dataset)\n",
    "fig.set_ylabel(\"Accuracy\")\n",
    "fig.set_xlabel(\"Number of Classes\")\n",
    "fig.get_figure().savefig(FIGURES_PATH / \"accuracies.pdf\", dpi=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = None\n",
    "for h in histories:\n",
    "    plt = sns.lineplot([1,2,3,4,5], h.history['accuracy'])\n",
    "plt.get_figure().show()\n",
    "plt.get_figure().savefig(FIGURES_PATH / '25_history.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
